from github import Github
import os
import json
import time
import openai
import nbformat
from nbformat.v4 import new_notebook, new_code_cell

open_ai_key = os.getenv("OPENAI_API_KEY")
github_key = os.getenv("GITHUB_ACCESS_TOKEN")
openai.api_key = open_ai_key
g = Github(github_key)
user_git = g.get_user()



###Function to create repo########

def create_repo(repo_name, description, user=user_git):
    timestamp = int(time.time())
    user.create_repo(f"{repo_name}-{timestamp}", description=description)
    return f"https://github.com/project-vending/{repo_name}-{timestamp}", {repo_name}-{timestamp}



###Function to create ipynb########
def get_ipynb(link,columns):
    query= f"""{{"Dataset Link": "{link}",
                "columns of the dataset": "{columns}",
                "prompt":"write code to Do eda on this dataset with check for nulls, plots, distribution etc. Also add a comment for  each code block so I can split the code into a Jupyter notebook"}}"""
    
    response =  openai.ChatCompletion.create(
        model = "gpt-3.5-turbo", 
        messages = [
            {"role" : "user", "content" : query }
        ]
    )
    response= response["choices"][0]["message"]["content"]

    #Break response into a list of code blocks by the # separator without removing the separator
    code_blocks = response.split('#')

    #Add # to the beginning of each code block
    code_blocks = ['#' + code_block for code_block in code_blocks]

    # Create a new notebook
    notebook = new_notebook()

    for code in code_blocks:
        cell = new_code_cell(code)
        notebook['cells'].append(cell)


    # Save the notebook to a file
    nbformat.write(notebook, 'EDA.ipynb')
    #Return the notebook
    return notebook


### Function to push the notebook###
def push_notebook(repo_name, message, notebook, filename): 
    timestamp = str(time.time())
    notebook = json.dumps(notebook)
    repo = user_git.get_repo(repo_name)
    repo.create_file(f"{filename}.ipynb", message, notebook)
    
    
# jd = '''Knowledge Of The Following Is An Advantage

# As a team member, you will also work on production support issues and be involved in improvements on CI/CD automation and DevOps.

# Kafka, AWS SQS, or MQ
# Relational and non-relational Databases (Postgres, SQL Server, Databricks)
# Spark
# Vertx
# Spring
# Flask
# Jenkins
# AWS
# '''

# # query = f"Can you provide me 1 complete personal project that I can add to my resume and the code can be generated by you for the job description: {jd}. Give the response in this json format "

# # response =  openai.ChatCompletion.create(
# #         model = "gpt-3.5-turbo", 
# #         messages = [
# #             {"role" : "user", "content" : query }
# #         ]
# #     )
# # print(response["choices"][0]["message"]["content"])


# query = f'''project-root/ |-- app/ |   |-- __init__.py |   |-- main.py |   |-- schemas.py |   |-- settings.py |-- streamlit/ |   |-- app.py |   |-- requirements.txt |-- Dockerfile |-- docker-compose.yml |-- README.md generate code for these files according to the project - Snehil Aryan Real-time data processing: Create a real-time data processing application using FastAPI, which consumes data from a data source (e.g., Kafka) and writes the processed data to an AWS data store (e.g., DynamoDB, S3). Use Streamlit to create a dashboard that displays the real-time data and its trends. give me a starter file structures and files in each folder for this project'''

# response =  openai.ChatCompletion.create(
#         model = "gpt-3.5-turbo", 
#         messages = [
#             {"role" : "user", "content" : query }
#         ]
#     )
# print(response["choices"][0]["message"]["content"])


    
    
# # link = create_repo("testing-push-notebook", "This is the first functional test", user) 
# notebook = get_ipynb("https://raw.githubusercontent.com/midhunmohank/INFO6105/main/advertising.csv","TV,Radio,Newspaper,Sales")
# # print(link)
# push_notebook("testing-push-notebook", "News Paper EDA", notebook, "john")






